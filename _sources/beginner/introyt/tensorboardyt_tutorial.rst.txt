
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "beginner/introyt/tensorboardyt_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_beginner_introyt_tensorboardyt_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_introyt_tensorboardyt_tutorial.py:


`Introduction <introyt1_tutorial.html>`_ ||
`Tensors <tensors_deeper_tutorial.html>`_ ||
`Autograd <autogradyt_tutorial.html>`_ ||
`Building Models <modelsyt_tutorial.html>`_ ||
**TensorBoard Support** ||
`Training Models <trainingyt.html>`_ ||
`Model Understanding <captumyt.html>`_

PyTorch TensorBoard Support
===========================

Follow along with the video below or on `youtube <https://www.youtube.com/watch?v=6CEld3hZgqc>`__.

.. raw:: html

   <div style="margin-top:10px; margin-bottom:10px;">
     <iframe width="560" height="315" src="https://www.youtube.com/embed/6CEld3hZgqc" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
   </div>

Before You Start
----------------

To run this tutorial, you’ll need to install PyTorch, TorchVision,
Matplotlib, and TensorBoard.

With ``conda``::

    conda install pytorch torchvision -c pytorch
    conda install matplotlib tensorboard

With ``pip``::

    pip install torch torchvision matplotlib tensorboard

Once the dependencies are installed, restart this notebook in the Python
environment where you installed them.


Introduction
------------
 
In this notebook, we’ll be training a variant of LeNet-5 against the
Fashion-MNIST dataset. Fashion-MNIST is a set of image tiles depicting
various garments, with ten class labels indicating the type of garment
depicted. 

.. GENERATED FROM PYTHON SOURCE LINES 49-68

.. code-block:: default


    # PyTorch model and training necessities
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim

    # Image datasets and image manipulation
    import torchvision
    import torchvision.transforms as transforms

    # Image display
    import matplotlib.pyplot as plt
    import numpy as np

    # PyTorch TensorBoard support
    from torch.utils.tensorboard import SummaryWriter









.. GENERATED FROM PYTHON SOURCE LINES 69-74

Showing Images in TensorBoard
-----------------------------

Let’s start by adding sample images from our dataset to TensorBoard:


.. GENERATED FROM PYTHON SOURCE LINES 74-125

.. code-block:: default


    # Gather datasets and prepare them for consumption
    transform = transforms.Compose(
        [transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))])

    # Store separate training and validations splits in ./data
    training_set = torchvision.datasets.FashionMNIST('./data',
        download=True,
        train=True,
        transform=transform)
    validation_set = torchvision.datasets.FashionMNIST('./data',
        download=True,
        train=False,
        transform=transform)

    training_loader = torch.utils.data.DataLoader(training_set,
                                                  batch_size=4,
                                                  shuffle=True,
                                                  num_workers=2)


    validation_loader = torch.utils.data.DataLoader(validation_set,
                                                    batch_size=4,
                                                    shuffle=False,
                                                    num_workers=2)

    # Class labels
    classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')

    # Helper function for inline image display
    def matplotlib_imshow(img, one_channel=False):
        if one_channel:
            img = img.mean(dim=0)
        img = img / 2 + 0.5     # unnormalize
        npimg = img.numpy()
        if one_channel:
            plt.imshow(npimg, cmap="Greys")
        else:
            plt.imshow(np.transpose(npimg, (1, 2, 0)))

    # Extract a batch of 4 images
    dataiter = iter(training_loader)
    images, labels = next(dataiter)

    # Create a grid from the images and show them
    img_grid = torchvision.utils.make_grid(images)
    matplotlib_imshow(img_grid, one_channel=True)





.. image-sg:: /beginner/introyt/images/sphx_glr_tensorboardyt_tutorial_001.png
   :alt: tensorboardyt tutorial
   :srcset: /beginner/introyt/images/sphx_glr_tensorboardyt_tutorial_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz

      0%|          | 0/26421880 [00:00<?, ?it/s]
      0%|          | 32768/26421880 [00:00<01:28, 299765.00it/s]
      0%|          | 65536/26421880 [00:00<01:28, 298134.01it/s]
      0%|          | 131072/26421880 [00:00<01:00, 433909.06it/s]
      1%|          | 196608/26421880 [00:00<00:52, 498047.03it/s]
      1%|1         | 393216/26421880 [00:00<00:27, 963492.05it/s]
      2%|1         | 524288/26421880 [00:00<00:24, 1041187.19it/s]
      3%|2         | 753664/26421880 [00:00<00:18, 1383456.42it/s]
      4%|3         | 1015808/26421880 [00:00<00:14, 1701070.32it/s]
      5%|4         | 1277952/26421880 [00:00<00:13, 1914346.17it/s]
      6%|5         | 1507328/26421880 [00:01<00:12, 1968275.78it/s]
      7%|6         | 1769472/26421880 [00:01<00:11, 2091815.20it/s]
      8%|7         | 2031616/26421880 [00:01<00:11, 2173363.77it/s]
      9%|8         | 2260992/26421880 [00:01<00:11, 2158431.46it/s]
     10%|9         | 2523136/26421880 [00:01<00:10, 2217576.06it/s]
     11%|#         | 2785280/26421880 [00:01<00:10, 2263804.74it/s]
     12%|#1        | 3047424/26421880 [00:01<00:10, 2300112.44it/s]
     13%|#2        | 3309568/26421880 [00:01<00:09, 2322119.91it/s]
     14%|#3        | 3571712/26421880 [00:01<00:09, 2341271.28it/s]
     15%|#4        | 3833856/26421880 [00:02<00:09, 2351235.37it/s]
     16%|#5        | 4096000/26421880 [00:02<00:09, 2357896.66it/s]
     16%|#6        | 4358144/26421880 [00:02<00:09, 2366281.45it/s]
     17%|#7        | 4620288/26421880 [00:02<00:09, 2372107.64it/s]
     18%|#8        | 4882432/26421880 [00:02<00:09, 2373255.02it/s]
     19%|#9        | 5144576/26421880 [00:02<00:08, 2377368.75it/s]
     20%|##        | 5406720/26421880 [00:02<00:08, 2379675.15it/s]
     21%|##1       | 5668864/26421880 [00:02<00:08, 2382365.92it/s]
     22%|##2       | 5931008/26421880 [00:02<00:08, 2383845.08it/s]
     23%|##3       | 6193152/26421880 [00:03<00:08, 2385106.20it/s]
     24%|##4       | 6455296/26421880 [00:03<00:08, 2388719.59it/s]
     25%|##5       | 6717440/26421880 [00:03<00:08, 2388433.56it/s]
     26%|##6       | 6979584/26421880 [00:03<00:08, 2387673.21it/s]
     27%|##7       | 7241728/26421880 [00:03<00:08, 2396294.79it/s]
     28%|##8       | 7503872/26421880 [00:03<00:07, 2395447.74it/s]
     29%|##9       | 7766016/26421880 [00:03<00:07, 2396521.48it/s]
     30%|###       | 8028160/26421880 [00:03<00:07, 2394180.64it/s]
     31%|###1      | 8290304/26421880 [00:03<00:07, 2394245.37it/s]
     32%|###2      | 8552448/26421880 [00:04<00:07, 2391361.46it/s]
     33%|###3      | 8847360/26421880 [00:04<00:07, 2472719.64it/s]
     34%|###4      | 9109504/26421880 [00:04<00:07, 2452260.75it/s]
     35%|###5      | 9371648/26421880 [00:04<00:07, 2433643.19it/s]
     37%|###6      | 9666560/26421880 [00:04<00:06, 2504912.98it/s]
     38%|###7      | 9961472/26421880 [00:04<00:06, 2554535.86it/s]
     39%|###8      | 10223616/26421880 [00:04<00:06, 2509662.35it/s]
     40%|###9      | 10518528/26421880 [00:04<00:06, 2557693.21it/s]
     41%|####      | 10813440/26421880 [00:04<00:06, 2595867.12it/s]
     42%|####2     | 11108352/26421880 [00:05<00:05, 2622949.48it/s]
     43%|####3     | 11403264/26421880 [00:05<00:05, 2641926.05it/s]
     44%|####4     | 11698176/26421880 [00:05<00:05, 2659917.34it/s]
     46%|####5     | 12025856/26421880 [00:05<00:05, 2746098.09it/s]
     47%|####6     | 12320768/26421880 [00:05<00:05, 2734791.67it/s]
     48%|####7     | 12648448/26421880 [00:05<00:04, 2803035.80it/s]
     49%|####9     | 12976128/26421880 [00:05<00:04, 2852320.37it/s]
     50%|#####     | 13303808/26421880 [00:05<00:04, 2887477.61it/s]
     52%|#####1    | 13631488/26421880 [00:05<00:04, 2921313.64it/s]
     53%|#####2    | 13959168/26421880 [00:06<00:04, 2944770.29it/s]
     54%|#####4    | 14286848/26421880 [00:06<00:04, 2964460.04it/s]
     55%|#####5    | 14647296/26421880 [00:06<00:03, 3052334.31it/s]
     57%|#####6    | 15007744/26421880 [00:06<00:03, 3112901.86it/s]
     58%|#####8    | 15368192/26421880 [00:06<00:03, 3158745.47it/s]
     60%|#####9    | 15728640/26421880 [00:06<00:03, 3202006.48it/s]
     61%|######    | 16089088/26421880 [00:06<00:03, 3237010.70it/s]
     62%|######2   | 16482304/26421880 [00:06<00:02, 3327998.69it/s]
     64%|######3   | 16875520/26421880 [00:06<00:02, 3397062.12it/s]
     65%|######5   | 17268736/26421880 [00:07<00:02, 3452246.65it/s]
     67%|######6   | 17661952/26421880 [00:07<00:02, 3499776.86it/s]
     68%|######8   | 18055168/26421880 [00:07<00:02, 3535746.91it/s]
     70%|######9   | 18481152/26421880 [00:07<00:02, 3629932.09it/s]
     72%|#######1  | 18907136/26421880 [00:07<00:02, 3705179.51it/s]
     73%|#######3  | 19365888/26421880 [00:07<00:01, 3826389.89it/s]
     75%|#######4  | 19791872/26421880 [00:07<00:01, 3853248.22it/s]
     77%|#######6  | 20250624/26421880 [00:07<00:01, 3950270.15it/s]
     78%|#######8  | 20709376/26421880 [00:07<00:01, 4018561.34it/s]
     80%|########  | 21200896/26421880 [00:08<00:01, 4141108.85it/s]
     82%|########1 | 21659648/26421880 [00:08<00:01, 4177797.37it/s]
     84%|########3 | 22151168/26421880 [00:08<00:00, 4271435.72it/s]
     86%|########5 | 22675456/26421880 [00:08<00:00, 4399350.81it/s]
     88%|########7 | 23199744/26421880 [00:08<00:00, 4503654.29it/s]
     90%|########9 | 23724032/26421880 [00:08<00:00, 4592708.96it/s]
     92%|#########1| 24248320/26421880 [00:08<00:00, 4656788.32it/s]
     94%|#########3| 24805376/26421880 [00:08<00:00, 4773985.63it/s]
     96%|#########5| 25362432/26421880 [00:08<00:00, 4871301.94it/s]
     98%|#########8| 25952256/26421880 [00:09<00:00, 5011947.83it/s]
    100%|##########| 26421880/26421880 [00:09<00:00, 2928094.83it/s]
    Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz

      0%|          | 0/29515 [00:00<?, ?it/s]
    100%|##########| 29515/29515 [00:00<00:00, 270868.77it/s]
    100%|##########| 29515/29515 [00:00<00:00, 269565.26it/s]
    Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz

      0%|          | 0/4422102 [00:00<?, ?it/s]
      1%|          | 32768/4422102 [00:00<00:14, 300060.16it/s]
      1%|1         | 65536/4422102 [00:00<00:14, 298936.07it/s]
      3%|2         | 131072/4422102 [00:00<00:09, 434862.29it/s]
      4%|4         | 196608/4422102 [00:00<00:08, 498871.46it/s]
      9%|8         | 393216/4422102 [00:00<00:04, 965005.57it/s]
     11%|#1        | 491520/4422102 [00:00<00:04, 937719.20it/s]
     17%|#7        | 753664/4422102 [00:00<00:02, 1415307.89it/s]
     30%|##9       | 1310720/4422102 [00:00<00:01, 2581144.53it/s]
     56%|#####5    | 2457600/4422102 [00:00<00:00, 5038900.46it/s]
    100%|##########| 4422102/4422102 [00:01<00:00, 4388340.18it/s]
    Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz

      0%|          | 0/5148 [00:00<?, ?it/s]
    100%|##########| 5148/5148 [00:00<00:00, 20408579.39it/s]
    Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw





.. GENERATED FROM PYTHON SOURCE LINES 126-131

Above, we used TorchVision and Matplotlib to create a visual grid of a
minibatch of our input data. Below, we use the ``add_image()`` call on
``SummaryWriter`` to log the image for consumption by TensorBoard, and
we also call ``flush()`` to make sure it’s written to disk right away.


.. GENERATED FROM PYTHON SOURCE LINES 131-145

.. code-block:: default


    # Default log_dir argument is "runs" - but it's good to be specific
    # torch.utils.tensorboard.SummaryWriter is imported above
    writer = SummaryWriter('runs/fashion_mnist_experiment_1')

    # Write image data to TensorBoard log dir
    writer.add_image('Four Fashion-MNIST Images', img_grid)
    writer.flush()

    # To view, start TensorBoard on the command line with:
    #   tensorboard --logdir=runs
    # ...and open a browser tab to http://localhost:6006/









.. GENERATED FROM PYTHON SOURCE LINES 146-160

If you start TensorBoard at the command line and open it in a new
browser tab (usually at `localhost:6006 <localhost:6006>`__), you should
see the image grid under the IMAGES tab.

Graphing Scalars to Visualize Training
--------------------------------------

TensorBoard is useful for tracking the progress and efficacy of your
training. Below, we’ll run a training loop, track some metrics, and save
the data for TensorBoard’s consumption.

Let’s define a model to categorize our image tiles, and an optimizer and
loss function for training:


.. GENERATED FROM PYTHON SOURCE LINES 160-186

.. code-block:: default


    class Net(nn.Module):
        def __init__(self):
            super(Net, self).__init__()
            self.conv1 = nn.Conv2d(1, 6, 5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = nn.Conv2d(6, 16, 5)
            self.fc1 = nn.Linear(16 * 4 * 4, 120)
            self.fc2 = nn.Linear(120, 84)
            self.fc3 = nn.Linear(84, 10)

        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = x.view(-1, 16 * 4 * 4)
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return x
    

    net = Net()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)









.. GENERATED FROM PYTHON SOURCE LINES 187-190

Now let’s train a single epoch, and evaluate the training vs. validation
set losses every 1000 batches:


.. GENERATED FROM PYTHON SOURCE LINES 190-232

.. code-block:: default


    print(len(validation_loader))
    for epoch in range(1):  # loop over the dataset multiple times
        running_loss = 0.0

        for i, data in enumerate(training_loader, 0):
            # basic training loop
            inputs, labels = data
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            if i % 1000 == 999:    # Every 1000 mini-batches...
                print('Batch {}'.format(i + 1))
                # Check against the validation set
                running_vloss = 0.0
            
                net.train(False) # Don't need to track gradents for validation
                for j, vdata in enumerate(validation_loader, 0):
                    vinputs, vlabels = vdata
                    voutputs = net(vinputs)
                    vloss = criterion(voutputs, vlabels)
                    running_vloss += vloss.item()
                net.train(True) # Turn gradients back on for training
            
                avg_loss = running_loss / 1000
                avg_vloss = running_vloss / len(validation_loader)
            
                # Log the running loss averaged per batch
                writer.add_scalars('Training vs. Validation Loss',
                                { 'Training' : avg_loss, 'Validation' : avg_vloss },
                                epoch * len(training_loader) + i)

                running_loss = 0.0
    print('Finished Training')

    writer.flush()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    2500
    Batch 1000
    Batch 2000
    Batch 3000
    Batch 4000
    Batch 5000
    Batch 6000
    Batch 7000
    Batch 8000
    Batch 9000
    Batch 10000
    Batch 11000
    Batch 12000
    Batch 13000
    Batch 14000
    Batch 15000
    Finished Training




.. GENERATED FROM PYTHON SOURCE LINES 233-242

Switch to your open TensorBoard and have a look at the SCALARS tab.

Visualizing Your Model
----------------------

TensorBoard can also be used to examine the data flow within your model.
To do this, call the ``add_graph()`` method with a model and sample
input. When you open


.. GENERATED FROM PYTHON SOURCE LINES 242-253

.. code-block:: default


    # Again, grab a single mini-batch of images
    dataiter = iter(training_loader)
    images, labels = next(dataiter)

    # add_graph() will trace the sample input through your model,
    # and render it as a graph.
    writer.add_graph(net, images)
    writer.flush()









.. GENERATED FROM PYTHON SOURCE LINES 254-271

When you switch over to TensorBoard, you should see a GRAPHS tab.
Double-click the “NET” node to see the layers and data flow within your
model.

Visualizing Your Dataset with Embeddings
----------------------------------------

The 28-by-28 image tiles we’re using can be modeled as 784-dimensional
vectors (28 \* 28 = 784). It can be instructive to project this to a
lower-dimensional representation. The ``add_embedding()`` method will
project a set of data onto the three dimensions with highest variance,
and display them as an interactive 3D chart. The ``add_embedding()``
method does this automatically by projecting to the three dimensions
with highest variance.

Below, we’ll take a sample of our data, and generate such an embedding:


.. GENERATED FROM PYTHON SOURCE LINES 271-294

.. code-block:: default


    # Select a random subset of data and corresponding labels
    def select_n_random(data, labels, n=100):
        assert len(data) == len(labels)

        perm = torch.randperm(len(data))
        return data[perm][:n], labels[perm][:n]

    # Extract a random subset of data
    images, labels = select_n_random(training_set.data, training_set.targets)

    # get the class labels for each image
    class_labels = [classes[label] for label in labels]

    # log embeddings
    features = images.view(-1, 28 * 28)
    writer.add_embedding(features,
                        metadata=class_labels,
                        label_img=images.unsqueeze(1))
    writer.flush()
    writer.close()









.. GENERATED FROM PYTHON SOURCE LINES 295-316

Now if you switch to TensorBoard and select the PROJECTOR tab, you
should see a 3D representation of the projection. You can rotate and
zoom the model. Examine it at large and small scales, and see whether
you can spot patterns in the projected data and the clustering of
labels.

For better visibility, it’s recommended to:

- Select “label” from the “Color by” drop-down on the left.
- Toggle the Night Mode icon along the top to place the
  light-colored images on a dark background.

Other Resources
---------------

For more information, have a look at:

- PyTorch documentation on `torch.utils.tensorboard.SummaryWriter <https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter>`__
- Tensorboard tutorial content in the `PyTorch.org Tutorials <https://pytorch.org/tutorials/>`__ 
- For more information about TensorBoard, see the `TensorBoard
  documentation <https://www.tensorflow.org/tensorboard>`__


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  40.625 seconds)


.. _sphx_glr_download_beginner_introyt_tensorboardyt_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tensorboardyt_tutorial.py <tensorboardyt_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tensorboardyt_tutorial.ipynb <tensorboardyt_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
